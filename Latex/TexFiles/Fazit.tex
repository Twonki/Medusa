\chapter{Fazit}
\label{cha:Fazit} \label{cha:Schluss}
Die vorangehende Arbeit stellt verschiedene bilderzeugende Verfahren zum Angriff einer Verkehrsschilder erkennenden \ac{KI} vor.\\
Die Motivation für die Arbeit ist das Forschungsfeld des autonomen Fahrens, in welchem \acp{KI} für die optische Erkennung der Umwelt, im besonderen für das Erkennen von Straßenschildern, verwendet werden sollen. Vorangehende Arbeiten mit Neuronalen Netzen zeigten, dass diese zum aktuellen Zeitpunkt anfällig für Manipulationen sind und fehlerhaft reagieren können. Das gezielte Ausnutzen dieser Fehleranfälligkeit wird in der Forschung als \textit{Adversarial Attack} bezeichnet. Das Ziel der Arbeit ist es verschiedene Verfahren vorzustellen und zu untersuchen, mit denen \textit{Adversarial Attacks} gegen ein \ac{NN} zur Erkennung von Straßenschildern durchgeführt werden können. Die Aufgabenstellung und das \ac{NN} werden von der \acl{GI} im Rahmen des Wettbewerbs \textit{InformatiCup 2019} gestellt.

Im ersten Schritt der Arbeit werden die Anforderungen an die bilderzeugenden Verfahren zum Angriff der \ac{KI} des \ac{GI}-Wettbewerbs analysiert und die Rahmenbedingungen ermittelt. Die Hauptquelle dafür ist die Aufgabenstellung des InformatiCups 2019, sowie die zur Verfügung stehende \ac{KI} des Wettbewerbs, welche über eine Webschnittstelle erreichbar ist.

In Kapitel \ref{cha:TechKonzept} werden die verwendeten Technologien der Arbeit vorgestellt. Diese dienen als Grundlage für die verschiedenen Verfahren in den Kapiteln \ref{cha:Degeneration} - \ref{cha:gascent}. Anschließend wird erläutert, warum sich \textit{Adversarial Attacks} für die Zielerreichung der Arbeit eignen. Der letzte Abschnitt des Kapitels behandelt die Implementierung eines eigenen Modells zur Klassifizierung von Straßenschildern, welches im Projekt \textit{Aphrodite} genannt wird. Das Modell wird in dem Verfahren lokale Degeneration in Kapitel \ref{cha:Degeneration} und im \textit{Saliency Maps} Verfahren in Kapitel \ref{cha:saliency} eingesetzt. Das eigene Modell dient als Ersatz für das \ac{NN} des \ac{GI}-Wettbewerbs, da Anfragen an letzteres Beschränkungen wie einer Anzahl an Requests pro Minute unterliegen. Zusätzlich bietet ein lokales Modell Möglichkeiten zur detaillierten Untersuchung der Verfahren, da es sich nicht um ein Black Box Modell handelt.
 
Das erste vorgestellte bilderzeugende Verfahren zum Angriff einer Verkehrsschilder erkennenden \ac{KI} ist die Degeneration. Bei diesem Verfahren wird iterativ ein Bild manipuliert, welches ein Objekt enthält, dass vom Menschen und dem \ac{NN} des \ac{GI}-Wettbewerbs als Straßenschild klassifiziert wird. Durch verschiedene Bildbearbeitungsfunktionen wird das Bild solange verändert, bis ein Mensch kein Straßenschild mehr erkennt. Bei jeder Iteration wird darauf geachtet, dass das veränderte Bild von der \ac{KI} weiterhin mit hoher Konfidenz einer Straßenschild-Klasse zugeordnet wird. Die Degeneration ist ein vergleichsweise simpler Algorithmus, welcher zuverlässig gute Ergebnisse liefert.\\ 
Der wesentliche Vorteil der Degeneration ist die Unabhängigkeit vom Modell, welches angegriffen werden soll. Es muss kein Transfermodell erstellt werden, um angepasste Angriffe zu erzeugen, sondern der Algorithmus erzeugt Angriffsbilder anhand der Antworten der Black Box \ac{KI} des \ac{GI}-Wettbewerbs.\\
Diese Tatsache stellt gleichzeitig einen Nachteil der Degeneration dar, da Anfragen an die Webschnittstelle des \ac{GI}-Wettbewerbs mehr Zeit benötigen als Anfragen an ein lokales \ac{NN}. Für einen Angriff, bei dem ein hoher Konfidenzwert für eine Straßenschild-Klasse erreicht werden soll, muss über eine Stunde gerechnet werden. Die Zeit erhöht sich weiter, wenn das angegriffene \ac{NN} dynamisch auf Rauschen reagiert und zugesendete Bilder für ein kontinuierliches Training nutzt. Da eine Grundvoraussetzung des Wettbewerbs jedoch ein abgeschlossenes trainiertes Netz ist, gehört dieses Problem nicht zum Rahmen der Arbeit. \\
Trotz des Zeitaufwands ist die Effektivität der Degeneration vergleichsweise hoch, obwohl es sich im Wesentlichen um einen Brute-Force Angriff handelt. 

~\newline Des Weiteren konnten mit den optimierten \textit{Saliency Map} Verfahren Erfolge in den geglätteten Varianten verbucht werden. Somit konnte bestätigt werden, dass die Visualisierung der relevanten Pixel für ein Bild mit hoher Konfidenz geeignet sind, um als \textit{minimale Beispiele} für das NN verwendet werden können. 
Die entsprechenden Verfahren erzeugten Täuschungen mit Konfidenzen >0.9, allerdings lassen sich keine Aussagen über die allgemeine Verlässlichkeit und Zielgerichtetheit treffen.

Zuletzt können auch mit dem \textit{Gradient Ascent} Verfahren sehr gute Ergebnisse erzielt werden. Für 10 von 43 Klassen können Täuschungen mit hohen Konfidenzen am \ac{NN} des \ac{GI}-Wettbewerbs erzielt werden. Es wird vermutet, dass die Ausbeute mit weiterer Optimierung vergrößert werden kann oder auch echte Bilder für die Erzeugung der Täuschungen verwendet werden können. 


\section{Diskussion}
Die Herausforderungen des Wettbewerbs wirken sich auf die Erfolge der Verfahren aus. Es wird vermutet das beide Verfahren \textit{Saliency Map} und \textit{Gradient Ascent} bessere Ergebnisse liefern könnten, wenn das verwendete Bild größer als $64\times64$ wäre. Des Weiteren kann nichts über die Validierungsgenauigkeit des \ac{NN} des Wettbewerbs gesagt werden, weshalb auch die nicht zielgerichteten Täuschungsbilder als gutes Ergebnis betitelt werden. 


Beim Vergleich des \textit{Saliency Map} und \textit{Gradient Ascent} Verfahrens kann das zuletzt genannte bevorzugt werden. 

~\newline Diese Methoden lassen sich schwer mit der Degeneration vergleichen - die schnelleren Erfolge werden voraussichtlich mithilfe der Degeneration erzeugt, da sie deutlich weniger Vorlauf benötigt.
Nachdem allerdings das entsprechende Umfeld (eigenes Netz, Bibliotheken und Code) erzeugt wurde, können innerhalb des \textit{Gradient Ascent} Foolings deutlich schneller zuverlässige Bilder erzeugt werden.  
%content 
%vergleich der ergebnisse (reproduzierbarkeit, gezielt/random, qualität der bilder) - einschränkungen probleme unserer ansätze, wichtig: eigene fehler oder einschränkungen der methodik erkennen.



\section{Weiterführende Arbeiten}~\newline 
Die Ergebnisse dieser Arbeit liefern weitere Ansätze für zukünftige Aufgaben. Zum einen können die verwendeten Ansätze individuell weiter optimiert werden, bezüglich des selbsterstellten lokalen \acl{NN} und der Algorithmik bzw. deren Parameter. Zum anderen können Verfahren entwickelt werden, welche sich aller Methoden gezielt bedienen. 


~\newline Eine insgesamt spannende Arbeit wäre die Anwendung von Adversarial Attacks auf Sprachassistenten. 
Vor allem die Degeneration kann bereits in ihrem jetzigen Zustand genutzt werden, um Störgeräusche zu erzeugen, welche dennoch als Schlüsselwörter erkannt werden und ein \textit{Smarthome} hacken. 

Auch die anderen Verfahren sind insbesondere geeignet, sollte das Modell offenliegen. Der Sprachassistent-Hersteller Mycroft setzt auf Open-Source und stellt dementsprechend auch das (allgemeine) Model bereit. 
Zu bemerken ist hierbei noch, dass der Nutzer innerhalb der ersten Aktionen den Sprachassistenten auf seine Aussprache konfiguriert.  

~\newline An der Degeneration können ebenfalls großflächige Weiterentwicklungen vorgenommen werden: 

Zum einen die Verwendung der Tree-Degeneration, zum anderen können Beschleunigung und Verfall der einzelnen Alternations eingebaut werden. Ebenso sollte ein kleines Script erstellt werden, welche die verschiedenen Manipulationsfunktionen kurz auslotet und dem Nutzer vorstellt. 

Als komplexere Weiterentwicklung können mithilfe der Degeneration \textit{Manipulationsvektoren} erstellt werden und in einem Raum abgebildet werden. Hierbei stellt jedes \textit{Rauschen} (bzw. Bilddifferenz) und die Score-Differenz einen Vektor dar, welcher in einem Raum abgebildet werden kann.   

Mithilfe dieser Vektoren könnte über statistische bzw. numerische Verfahren solch ein Vektor gefunden werden, welcher die größte Länge hat allerdings die geringste Score-Differenz aufweist. Angewandt auf das Urbild sollte dieser Vektor ein optimales Ergebnis erzielen.  

~\newline Einen weiteren Blick sollte man der Überführbarkeit der Angriffe von einem lokalen Model auf ein unbekanntes Model widmen, wie es in Abschnitt \ref{sec:TrasiModell} thematisiert wird: 

Diese Eigenschaft werden von den Saliency-Maps und dem \textit{Gradient Ascent} Fooling hinreichend erfüllt, wohingegen solche Versuche beim Degeneration-Ver"-fahren scheitern. 

Im Rahmen dieses Ergebnisses könnte man einen gezielten Test der Verfahren auf zwei bekannte Modelle durchführen, um hier transparentere Werte zu erhalten und Gründe für dieses Verhalten auszumachen. 
%content
%Vergleich mit einem Framework, wie zum Beispiel CleverHans \footnote, Vergleich von einem gut und schlecht trainierten Modell, wie arg unterscheiden sich daraus generierte Bilder (lohnt es sich), wie wichtig ist die Bildgröße, 