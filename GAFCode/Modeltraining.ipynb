{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch-Training\n",
    "This file trains a PyTorchModel for use in Gradient ascent fooling. \n",
    "\n",
    "this was necessary as there are no similiar libraries for tensorflow at the moment.\n",
    "\n",
    "This Model is not build from scratch, but starts with the famous alex-net. \n",
    "\n",
    "This is just for documentation, please just use the model provided with the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "IMG_SIZE = 64\n",
    "NUM_CLASSES = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "def prepare_dataset(folder):\n",
    "    train_zip = folder + '/train_images.zip'\n",
    "    test_zip = folder + '/test_images.zip'\n",
    "    if not os.path.exists(train_zip) or not os.path.exists(test_zip):\n",
    "        raise(RuntimeError(\"Could not find \" + train_zip + \" and \" + test_zip))\n",
    "\n",
    "    # extract train_data.zip to train_data\n",
    "    train_folder = folder + '/train_images'\n",
    "\n",
    "    if not os.path.isdir(train_folder):\n",
    "        print(train_folder + ' not found, extracting ' + train_zip)\n",
    "        zip_ref = zipfile.ZipFile(train_zip, 'r')\n",
    "        zip_ref.extractall(folder)\n",
    "        zip_ref.close()\n",
    "\n",
    "    # extract test_data.zip to test_data\n",
    "    test_folder = folder + '/test_images'\n",
    "    if not os.path.isdir(test_folder):\n",
    "        print(test_folder + ' not found, extracting ' + test_zip)\n",
    "        zip_ref = zipfile.ZipFile(test_zip, 'r')\n",
    "        zip_ref.extractall(folder)\n",
    "        zip_ref.close()\n",
    "\n",
    "    # make validation_data by using images 00000*, 00001* and 00002* in each class\n",
    "    val_folder = folder + '/val_images'\n",
    "    if not os.path.isdir(val_folder):\n",
    "        print(val_folder + ' not found, making a validation set')\n",
    "        os.mkdir(val_folder)\n",
    "        for dirs in os.listdir(train_folder):\n",
    "            if dirs.startswith('000'):\n",
    "                os.mkdir(val_folder + '/' + dirs)\n",
    "                for f in os.listdir(train_folder + '/' + dirs):\n",
    "                    if f.startswith('00000') or f.startswith('00001') or f.startswith('00002'):\n",
    "                        # move file to validation folder\n",
    "                        os.rename(train_folder + '/' + dirs + '/' + f, val_folder + '/' + dirs + '/' + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "        else:\n",
    "            target = target.cpu()\n",
    "\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "\n",
    "            out_train = ('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5)\n",
    "\n",
    "            with open(\"######Progress-training.txt\", \"a\") as myfile:\n",
    "                myfile.write(str(datetime.datetime.now()) + \" -- \" + out_train + \"\\n\")\n",
    "            print(out_train)\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "        else:\n",
    "            target = target.cpu()\n",
    "\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            out_val = ('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5)\n",
    "            with open(\"######Progress-validation.txt\", \"a\") as myfile:\n",
    "                myfile.write(str(datetime.datetime.now()) + \" -- \" + out_val + \"\\n\")\n",
    "            print(out_val)\n",
    "\n",
    "    out_val_final = (' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    with open(\"######Progress-validation-final.txt\", \"a\") as myfile:\n",
    "        myfile.write(str(datetime.datetime.now()) + \" -- \" + out_val_final + \"\\n\")\n",
    "    print(out_val)\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth'):\n",
    "    #torch.save(state, filename)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "#   lr = 0.1 * (0.1 ** (epoch // 30)) # first value is learning rate (1st trial)\n",
    "#    lr = 0.01 * (0.1 ** (epoch // 30)) # first value is learning rate (2nd trial)\n",
    "#    lr = 0.001 * (0.1 ** (epoch // 30)) # first value is learning rate (3rd trial)\n",
    "    lr = 0.01 * (0.1 ** (epoch // 30)) # first value is learning rate (4th trial)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Start training epoch: 1/50\n",
      "@ 2018-12-21 05:47:54.137344\n",
      "============================\n",
      "Epoch: [1][0/354]\tTime 3.239 (3.239)\tData 0.182 (0.182)\tLoss 3.7597 (3.7597)\tPrec@1 0.000 (0.000)\tPrec@5 13.000 (13.000)\n",
      "Epoch: [1][10/354]\tTime 2.345 (2.368)\tData 0.003 (0.021)\tLoss 3.7405 (3.7517)\tPrec@1 3.000 (3.818)\tPrec@5 25.000 (20.636)\n",
      "Epoch: [1][20/354]\tTime 2.284 (2.363)\tData 0.005 (0.013)\tLoss 3.7148 (3.7386)\tPrec@1 4.000 (4.810)\tPrec@5 22.000 (23.952)\n",
      "Epoch: [1][30/354]\tTime 2.310 (2.358)\tData 0.002 (0.010)\tLoss 3.6753 (3.7217)\tPrec@1 3.000 (4.710)\tPrec@5 21.000 (25.323)\n",
      "Epoch: [1][40/354]\tTime 2.271 (2.344)\tData 0.002 (0.008)\tLoss 3.5673 (3.6931)\tPrec@1 1.000 (5.244)\tPrec@5 25.000 (26.146)\n",
      "Epoch: [1][50/354]\tTime 2.295 (2.333)\tData 0.003 (0.007)\tLoss 3.5155 (3.6469)\tPrec@1 1.000 (5.471)\tPrec@5 22.000 (26.333)\n",
      "Epoch: [1][60/354]\tTime 2.273 (2.326)\tData 0.002 (0.006)\tLoss 3.4240 (3.6124)\tPrec@1 8.000 (5.508)\tPrec@5 29.000 (26.410)\n",
      "Epoch: [1][70/354]\tTime 2.223 (2.333)\tData 0.002 (0.006)\tLoss 3.4641 (3.5922)\tPrec@1 8.000 (5.620)\tPrec@5 29.000 (26.761)\n",
      "Epoch: [1][80/354]\tTime 2.254 (2.336)\tData 0.005 (0.005)\tLoss 3.4475 (3.5768)\tPrec@1 4.000 (5.593)\tPrec@5 28.000 (26.679)\n",
      "Epoch: [1][90/354]\tTime 2.497 (2.334)\tData 0.005 (0.005)\tLoss 3.3225 (3.5588)\tPrec@1 9.000 (5.813)\tPrec@5 30.000 (26.945)\n",
      "Epoch: [1][100/354]\tTime 2.350 (2.336)\tData 0.004 (0.005)\tLoss 3.3897 (3.5486)\tPrec@1 9.000 (5.851)\tPrec@5 35.000 (27.238)\n",
      "Epoch: [1][110/354]\tTime 2.472 (2.337)\tData 0.002 (0.005)\tLoss 3.4723 (3.5373)\tPrec@1 5.000 (6.045)\tPrec@5 26.000 (27.414)\n",
      "Epoch: [1][120/354]\tTime 2.284 (2.336)\tData 0.002 (0.004)\tLoss 3.1957 (3.5262)\tPrec@1 11.000 (6.174)\tPrec@5 35.000 (27.744)\n",
      "Epoch: [1][130/354]\tTime 2.391 (2.339)\tData 0.006 (0.004)\tLoss 3.3118 (3.5153)\tPrec@1 8.000 (6.137)\tPrec@5 29.000 (27.802)\n",
      "Epoch: [1][140/354]\tTime 2.238 (2.338)\tData 0.002 (0.004)\tLoss 3.4478 (3.5073)\tPrec@1 4.000 (6.149)\tPrec@5 25.000 (27.830)\n",
      "Epoch: [1][150/354]\tTime 2.293 (2.337)\tData 0.002 (0.004)\tLoss 3.3268 (3.4990)\tPrec@1 7.000 (6.192)\tPrec@5 31.000 (27.927)\n",
      "Epoch: [1][160/354]\tTime 2.269 (2.336)\tData 0.003 (0.004)\tLoss 3.3417 (3.4894)\tPrec@1 5.000 (6.180)\tPrec@5 30.000 (28.087)\n",
      "Epoch: [1][170/354]\tTime 2.403 (2.336)\tData 0.005 (0.004)\tLoss 3.4500 (3.4810)\tPrec@1 8.000 (6.333)\tPrec@5 29.000 (28.287)\n",
      "Epoch: [1][180/354]\tTime 2.558 (2.338)\tData 0.005 (0.004)\tLoss 3.2951 (3.4738)\tPrec@1 7.000 (6.431)\tPrec@5 38.000 (28.459)\n",
      "Epoch: [1][190/354]\tTime 2.410 (2.338)\tData 0.004 (0.004)\tLoss 3.3934 (3.4667)\tPrec@1 7.000 (6.592)\tPrec@5 31.000 (28.733)\n",
      "Epoch: [1][200/354]\tTime 2.290 (2.338)\tData 0.002 (0.004)\tLoss 3.1954 (3.4571)\tPrec@1 10.000 (6.736)\tPrec@5 41.000 (29.080)\n",
      "Epoch: [1][210/354]\tTime 2.301 (2.336)\tData 0.003 (0.004)\tLoss 3.3200 (3.4508)\tPrec@1 7.000 (6.829)\tPrec@5 38.000 (29.427)\n",
      "Epoch: [1][220/354]\tTime 2.257 (2.336)\tData 0.003 (0.004)\tLoss 3.3889 (3.4435)\tPrec@1 7.000 (6.955)\tPrec@5 30.000 (29.787)\n",
      "Epoch: [1][230/354]\tTime 2.315 (2.335)\tData 0.003 (0.004)\tLoss 3.3114 (3.4335)\tPrec@1 8.000 (7.173)\tPrec@5 29.000 (30.268)\n",
      "Epoch: [1][240/354]\tTime 2.276 (2.334)\tData 0.004 (0.004)\tLoss 3.0967 (3.4233)\tPrec@1 22.000 (7.436)\tPrec@5 47.000 (30.772)\n",
      "Epoch: [1][250/354]\tTime 2.345 (2.335)\tData 0.005 (0.004)\tLoss 3.3103 (3.4142)\tPrec@1 10.000 (7.625)\tPrec@5 37.000 (31.163)\n",
      "Epoch: [1][260/354]\tTime 2.219 (2.335)\tData 0.003 (0.004)\tLoss 3.1615 (3.4039)\tPrec@1 4.000 (7.747)\tPrec@5 37.000 (31.563)\n",
      "Epoch: [1][270/354]\tTime 2.319 (2.334)\tData 0.005 (0.004)\tLoss 3.1272 (3.3941)\tPrec@1 10.000 (7.886)\tPrec@5 43.000 (31.952)\n",
      "Epoch: [1][280/354]\tTime 2.315 (2.335)\tData 0.003 (0.004)\tLoss 3.2027 (3.3850)\tPrec@1 8.000 (7.993)\tPrec@5 39.000 (32.399)\n",
      "Epoch: [1][290/354]\tTime 2.444 (2.336)\tData 0.003 (0.004)\tLoss 3.1588 (3.3752)\tPrec@1 12.000 (8.110)\tPrec@5 47.000 (32.907)\n",
      "Epoch: [1][300/354]\tTime 2.403 (2.336)\tData 0.003 (0.004)\tLoss 2.9784 (3.3648)\tPrec@1 13.000 (8.272)\tPrec@5 59.000 (33.465)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "prepare_dataset('data')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "traindir = 'data/train_images'\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(traindir, transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )), batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "valdir = 'data/val_images'\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )), batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "from model import AlexNet\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet().to(device)\n",
    "use_sgd_optimizer = False\n",
    "\n",
    "if use_sgd_optimizer == True:\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                0.01, # learning rate (2nd trial)\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-6) # decay (2nd trial)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#####################\n",
    "epochs = 51 # trains for 50 epochs\n",
    "best_prec1 = 0\n",
    "#####################\n",
    "for epoch in range(1, epochs):\n",
    "    progress_epochs = \"\\n============================\\nStart training epoch: \" + str(epoch) + \"/\" + str(epochs-1) + \"\\n@ \" + str(datetime.datetime.now()) + \"\\n============================\"\n",
    "    with open(\"######Progress-epochs.txt\", \"a\") as myfile:\n",
    "        myfile.write(progress_epochs + \"\\n\")\n",
    "\n",
    "    print(progress_epochs)    \n",
    "\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': \"alexnet\",\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "    }, is_best)\n",
    "\n",
    "    progress_epochs = \"\\n=============================\\nFinished training epoch: \" + str(epoch) + \"/\" + str(epochs-1) + \"\\n@ \" + str(datetime.datetime.now()) + \"\\n=============================\"\n",
    "\n",
    "    with open(\"######Progress-epochs.txt\", \"a\") as myfile:\n",
    "        myfile.write(progress_epochs + \"\\n\")\n",
    "    print(progress_epochs)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
